{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PF_W2i6zKeK9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from haversine import haversine\n",
        "from datetime import datetime\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "TARGET = 'is_fraud'\n",
        "test_ids = test['id']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = [\n",
        "    'trans_num','first','last','street','city','state','zip',\n",
        "    'job','merchant','trans_date','trans_time','unix_time','dob'\n",
        "]\n",
        "\n",
        "X = train.drop(cols_to_drop + [TARGET], axis=1)\n",
        "y = train[TARGET]\n",
        "X_test = test.drop(cols_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "train_full = pd.read_csv('train.csv')\n",
        "test_full = pd.read_csv('test.csv')\n",
        "train_full['datetime'] = pd.to_datetime(train_full['trans_date'] + ' ' + train_full['trans_time'])\n",
        "test_full['datetime'] = pd.to_datetime(test_full['trans_date'] + ' ' + test_full['trans_time'])\n",
        "\n",
        "# Time Features\n",
        "X['hour'] = train_full['datetime'].dt.hour\n",
        "X['day_of_week'] = train_full['datetime'].dt.weekday\n",
        "X['is_night'] = ((X['hour'] >= 22) | (X['hour'] < 6)).astype(int)\n",
        "\n",
        "X_test['hour'] = test_full['datetime'].dt.hour\n",
        "X_test['day_of_week'] = test_full['datetime'].dt.weekday\n",
        "X_test['is_night'] = ((X_test['hour'] >= 22) | (X_test['hour'] < 6)).astype(int)\n",
        "\n",
        "def calculate_distance(lat1, long1, lat2, long2):\n",
        "    return haversine((lat1, long1), (lat2, long2))\n",
        "\n",
        "X['distance'] = X.apply(lambda r: calculate_distance(r['lat'], r['long'], r['merch_lat'], r['merch_long']), axis=1)\n",
        "X_test['distance'] = X_test.apply(lambda r: calculate_distance(r['lat'], r['long'], r['merch_lat'], r['merch_long']), axis=1)\n",
        "\n",
        "X['amt_per_citypop'] = X['amt'] / (X['city_pop'] + 1)\n",
        "X_test['amt_per_citypop'] = X_test['amt'] / (X_test['city_pop'] + 1)\n",
        "\n",
        "# Merchant location clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "X['merch_cluster'] = kmeans.fit_predict(X[['merch_lat','merch_long']])\n",
        "X_test['merch_cluster'] = kmeans.predict(X_test[['merch_lat','merch_long']])\n",
        "\n",
        "# Cardholder behavioral clusters - cluster cardholders by lat/long\n",
        "if 'cc_num' in X.columns:\n",
        "    cc_kmeans = KMeans(n_clusters=20, random_state=42)\n",
        "    # For clustering cardholders, assume lat/long represent them somehow\n",
        "    # This is simplistic; consider avg transaction amt, frequency, etc.\n",
        "    # We'll just cluster on their home location lat/long.\n",
        "    # Group by cc_num to get unique cardholder location (mean lat/long)\n",
        "    cardholder_loc = train_full.groupby('cc_num')[['lat','long']].mean().reset_index()\n",
        "    cc_labels = cc_kmeans.fit_predict(cardholder_loc[['lat','long']])\n",
        "    cc_map = dict(zip(cardholder_loc['cc_num'], cc_labels))\n",
        "    X['cardholder_cluster'] = X['cc_num'].map(cc_map)\n",
        "    X_test['cardholder_cluster'] = X_test['cc_num'].map(cc_map)\n",
        "# Get indices of NaN values in 'cardholder_cluster'\n",
        "    nan_indices = X_test['cardholder_cluster'].isnull()\n",
        "# Predict cluster labels for NaN values\n",
        "    predicted_clusters = cc_kmeans.predict(X_test.loc[nan_indices, ['lat', 'long']])\n",
        "# Fill NaN values with predicted clusters\n",
        "    X_test.loc[nan_indices, 'cardholder_cluster'] = predicted_clusters\n",
        "\n",
        "# Interaction features\n",
        "X['amt_distance_ratio'] = X['amt'] / (X['distance'] + 0.001)\n",
        "X_test['amt_distance_ratio'] = X_test['amt'] / (X_test['distance'] + 0.001)\n",
        "\n",
        "# Frequency of cc_num\n",
        "if 'cc_num' in X.columns:\n",
        "    cc_freq = X['cc_num'].value_counts()\n",
        "    X['cc_freq'] = X['cc_num'].map(cc_freq)\n",
        "    X_test['cc_freq'] = X_test['cc_num'].map(cc_freq).fillna(cc_freq.min())\n",
        "\n",
        "    # Aggregates by cc_num\n",
        "    agg_df = train_full.copy()\n",
        "    agg_df['is_night'] = ((pd.to_datetime(agg_df['trans_time']).dt.hour >= 22) |\n",
        "                          (pd.to_datetime(agg_df['trans_time']).dt.hour < 6)).astype(int)\n",
        "    agg_features = agg_df.groupby('cc_num').agg({\n",
        "        'amt': ['mean','std'],\n",
        "        'is_night':['mean'],\n",
        "    })\n",
        "    agg_features.columns = ['cc_amt_mean','cc_amt_std','cc_night_frac']\n",
        "    agg_features = agg_features.reset_index()\n",
        "\n",
        "    X = X.merge(agg_features, on='cc_num', how='left')\n",
        "    X_test = X_test.merge(agg_features, on='cc_num', how='left')\n",
        "\n",
        "    # Drop cc_num if you prefer\n",
        "    X = X.drop('cc_num', axis=1, errors='ignore')\n",
        "    X_test = X_test.drop('cc_num', axis=1, errors='ignore')\n",
        "\n",
        "## Additional outlier score\n",
        "# Get numeric features\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "iso = IsolationForest(n_estimators=100, random_state=42, contamination='auto')\n",
        "# Fit only on numeric features after imputing NaN values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # or other strategies like 'median'\n",
        "X_imp = imputer.fit_transform(X[numeric_cols])\n",
        "iso.fit(X_imp)\n",
        "X['outlier_score'] = iso.score_samples(X_imp)\n",
        "\n",
        "# Impute NaN values in X_test before scoring\n",
        "X_test_imp = imputer.transform(X_test[numeric_cols])\n",
        "X_test['outlier_score'] = iso.score_samples(X_test_imp)\n",
        "\n",
        "# PCA\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Impute NaN values before applying PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # or other strategies like 'median'\n",
        "X_imp = imputer.fit_transform(X[numeric_cols])\n",
        "X_test_imp = imputer.transform(X_test[numeric_cols])\n",
        "\n",
        "pca = PCA(n_components=5, random_state=42)\n",
        "pca_train = pca.fit_transform(X_imp) # Use imputed data for PCA\n",
        "pca_test = pca.transform(X_test_imp) # Use imputed data for PCA\n",
        "\n",
        "for i in range(5):\n",
        "    X[f'pca_{i}'] = pca_train[:, i]\n",
        "    X_test[f'pca_{i}'] = pca_test[:, i]\n"
      ],
      "metadata": {
        "id": "ZjdvO-XQLEZR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical-like columns for target encoding\n",
        "te_cols = ['merch_cluster','cardholder_cluster'] if 'cardholder_cluster' in X.columns else ['merch_cluster']\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for tcol in te_cols:\n",
        "    if tcol in X.columns:\n",
        "        X[tcol + '_te'] = 0\n",
        "        for trn_idx, val_idx in folds.split(X, y):\n",
        "            trn_x, val_x = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "            trn_y = y.iloc[trn_idx]\n",
        "            trn_fold = pd.concat([trn_x, trn_y], axis=1)\n",
        "            means = trn_fold.groupby(tcol)[TARGET].mean()\n",
        "            X.loc[val_idx, tcol + '_te'] = val_x[tcol].map(means)\n",
        "        full_train = pd.concat([X, y], axis=1)\n",
        "        full_means = full_train.groupby(tcol)[tcol + '_te'].mean()\n",
        "        X_test[tcol + '_te'] = X_test[tcol].map(full_means).fillna(y.mean())\n"
      ],
      "metadata": {
        "id": "r7tiZI42LKFw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numeric features\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "#Apply SimpleImputer to numeric columns only\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imp = imputer.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Impute NaN values in X_test before scoring\n",
        "X_test_imp = imputer.transform(X_test[numeric_cols])\n",
        "\n",
        "#Convert numpy arrays back to pandas DataFrames\n",
        "X = pd.DataFrame(X_imp, columns=numeric_cols, index=X.index)\n",
        "X_test = pd.DataFrame(X_test_imp, columns=numeric_cols, index=X_test.index)\n",
        "\n",
        "#Merge numeric features back with original DataFrame, keeping non-numeric columns unchanged\n",
        "X = X.merge(train.drop(columns=numeric_cols + cols_to_drop + [TARGET], axis=1, errors='ignore'), left_index=True, right_index=True)\n",
        "X_test = X_test.merge(test.drop(columns=numeric_cols + cols_to_drop, axis=1, errors='ignore'), left_index=True, right_index=True)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y)\n"
      ],
      "metadata": {
        "id": "A4Zlu220LPWQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = {\n",
        "    'xgb': XGBClassifier(n_estimators=600, max_depth=7, learning_rate=0.03, subsample=0.8,\n",
        "                         colsample_bytree=0.8, scale_pos_weight=5, random_state=42,\n",
        "                         use_label_encoder=False, eval_metric='logloss',\n",
        "                          enable_categorical=True),\n",
        "    'lgb': LGBMClassifier(n_estimators=600, max_depth=7, learning_rate=0.03, subsample=0.8,\n",
        "                          colsample_bytree=0.8, class_weight='balanced', random_state=42),\n",
        "    'cat': CatBoostClassifier(iterations=600, depth=7, learning_rate=0.03, subsample=0.8,\n",
        "                              auto_class_weights='Balanced', verbose=False, random_state=42),\n",
        "    'rf': RandomForestClassifier(n_estimators=300, max_depth=10, class_weight='balanced_subsample', random_state=42),\n",
        "    'et': ExtraTreesClassifier(n_estimators=300, max_depth=10, class_weight='balanced_subsample', random_state=42)\n",
        "}\n",
        "\n",
        "# Before fitting your XGBoost model, convert 'category' and 'gender' to categorical dtype:\n",
        "for col in ['category', 'gender']:\n",
        "    if col in X_train.columns:\n",
        "        X_train[col] = X_train[col].astype('category')\n",
        "        X_val[col] = X_val[col].astype('category')\n",
        "        X_test[col] = X_test[col].astype('category')\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros((len(X_train), len(base_models)))\n",
        "val_preds = {}\n",
        "test_preds = {}\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "for i, (name, model) in enumerate(base_models.items()):\n",
        "    oof = np.zeros(len(X_train))\n",
        "    val_pred_fold = np.zeros(len(X_val))\n",
        "    test_pred_fold = np.zeros((len(X_test), 5))\n",
        "\n",
        "    for f, (trn_idx, oof_idx) in enumerate(folds.split(X_train, y_train)):\n",
        "        trn_x, oof_x_ = X_train.iloc[trn_idx], X_train.iloc[oof_idx]\n",
        "        trn_y_f, oof_y_ = y_train.iloc[trn_idx], y_train.iloc[oof_idx]\n",
        "\n",
        "        # One-hot encoding for categorical features for all models including XGBoost before training\n",
        "        trn_x = pd.get_dummies(trn_x, columns=['category', 'gender'], drop_first=True)\n",
        "        oof_x_ = pd.get_dummies(oof_x_, columns=['category', 'gender'], drop_first=True)\n",
        "\n",
        "        # One-hot encoding for X_val and X_test before prediction\n",
        "        X_val_encoded = pd.get_dummies(X_val, columns=['category', 'gender'], drop_first=True)\n",
        "        X_test_encoded = pd.get_dummies(X_test, columns=['category', 'gender'], drop_first=True)\n",
        "\n",
        "        # Pass categorical features to CatBoost if it's the CatBoost model\n",
        "        if name == 'cat':\n",
        "            cat_features = trn_x.select_dtypes(include=['category']).columns.tolist()\n",
        "            model.fit(trn_x, trn_y_f, cat_features=cat_features)\n",
        "        else:\n",
        "            model.fit(trn_x, trn_y_f)\n",
        "\n",
        "        oof[oof_idx] = model.predict_proba(oof_x_)[:,1]\n",
        "        # Use one-hot encoded validation and test sets for prediction\n",
        "        val_pred_fold += model.predict_proba(X_val_encoded)[:,1] / folds.n_splits\n",
        "        test_pred_fold[:,f] = model.predict_proba(X_test_encoded)[:,1]\n",
        "\n",
        "    oof_preds[:, i] = oof\n",
        "    val_preds[name] = val_pred_fold\n",
        "    test_preds[name] = test_pred_fold.mean(axis=1)"
      ],
      "metadata": {
        "id": "lwLLG2jhLSEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99136279-a56e-4fb9-f465-0137571c4925"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 27071, number of negative: 210178\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176342 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5472\n",
            "[LightGBM] [Info] Number of data points in the train set: 237249, number of used features: 42\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 27071, number of negative: 210178\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088079 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5471\n",
            "[LightGBM] [Info] Number of data points in the train set: 237249, number of used features: 42\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 27072, number of negative: 210178\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090833 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5472\n",
            "[LightGBM] [Info] Number of data points in the train set: 237250, number of used features: 42\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 27071, number of negative: 210179\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095989 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5472\n",
            "[LightGBM] [Info] Number of data points in the train set: 237250, number of used features: 42\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 27071, number of negative: 210179\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093735 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5472\n",
            "[LightGBM] [Info] Number of data points in the train set: 237250, number of used features: 42\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = list(base_models.keys())\n",
        "# Let's just do a random weighting approach (in practice, try a more systematic approach)\n",
        "best_f1 = 0\n",
        "best_weights = None\n",
        "\n",
        "import itertools\n",
        "weights_grid = [0.25,0.5,1,1.5,2]\n",
        "\n",
        "for combo in itertools.product(weights_grid, repeat=len(model_names)):\n",
        "    combo = list(combo)\n",
        "    combined_val = np.zeros_like(val_preds[model_names[0]])\n",
        "    total_weight = sum(combo)\n",
        "    for w, m in zip(combo, model_names):\n",
        "        combined_val += val_preds[m]*w\n",
        "    combined_val /= total_weight\n",
        "\n",
        "    score = f1_score(y_val, (combined_val>0.5).astype(int))\n",
        "    if score > best_f1:\n",
        "        best_f1 = score\n",
        "        best_weights = combo\n",
        "\n",
        "print(\"Best weights:\", best_weights, \"F1:\", best_f1)\n",
        "\n",
        "weighted_oof = np.zeros_like(oof_preds[:,0])\n",
        "total_weight = sum(best_weights)\n",
        "for w, i in zip(best_weights, range(len(model_names))):\n",
        "    weighted_oof += oof_preds[:, i]*w\n",
        "weighted_oof /= total_weight\n",
        "\n",
        "weighted_val = np.zeros_like(val_preds[model_names[0]])\n",
        "for w, m in zip(best_weights, model_names):\n",
        "    weighted_val += val_preds[m]*w\n",
        "weighted_val /= total_weight\n",
        "\n",
        "weighted_test = np.zeros_like(test_preds[model_names[0]])\n",
        "for w, m in zip(best_weights, model_names):\n",
        "    weighted_test += test_preds[m]*w\n",
        "weighted_test /= total_weight\n",
        "\n",
        "meta_train_X = weighted_oof.reshape(-1,1)\n",
        "meta_val_X = weighted_val.reshape(-1,1)\n",
        "meta_test_X = weighted_test.reshape(-1,1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkBbvf2j5O_F",
        "outputId": "210f8310-1a86-44b3-bf8f-d807b5327612"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best weights: [2, 0.25, 0.25, 0.25, 0.25] F1: 0.8901708453516751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1','l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "grid = GridSearchCV(lr, param_grid, scoring='f1', cv=3, n_jobs=-1, verbose=1)\n",
        "grid.fit(meta_train_X, y_train)\n",
        "\n",
        "meta_model = grid.best_estimator_\n",
        "val_meta_prob = meta_model.predict_proba(meta_val_X)[:,1]\n",
        "\n",
        "# Threshold optimization\n",
        "thresholds = np.linspace(0,1,1000)\n",
        "best_threshold = 0.5\n",
        "best_f1_score_val = 0\n",
        "\n",
        "for thr in thresholds:\n",
        "    preds = (val_meta_prob > thr).astype(int)\n",
        "    score = f1_score(y_val, preds)\n",
        "    if score > best_f1_score_val:\n",
        "        best_f1_score_val = score\n",
        "        best_threshold = thr\n",
        "\n",
        "print(\"Best threshold:\", best_threshold, \"Best F1:\", best_f1_score_val)\n",
        "\n",
        "test_meta_prob = meta_model.predict_proba(meta_test_X)[:,1]\n",
        "test_final_pred = (test_meta_prob > best_threshold).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({'id': test_ids, 'is_fraud': test_final_pred})\n",
        "submission.to_csv('new_improved_model_submission.csv', index=False)\n",
        "print(\"Submission saved: new_improved_model_submission.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD4KFyEQEtSP",
        "outputId": "708f1c94-de40-4d3a-a496-e794d13be6b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best threshold: 0.8228228228228228 Best F1: 0.9094807050976655\n",
            "Submission saved: new_improved_model_submission.csv\n"
          ]
        }
      ]
    },
